---
title: Setting up a Scalable RStudio Instance in AWS
author: Sebastian Schweer
date: '2018-01-05'
categories:
  - AWS
  - R
tags:
  - Getting Started
  - RStudio AMI
slug: setting-up-a-scalable-rstudio-instance-in-aws
---

# Setting up an R Environment: Available Anytime, Anywhere?

 Assume you want to start to write `R` code (a very good decision, in my opinion) and you want to be able to write and test code whereever you are. Wouldn't it be awesome if one could set up an environment that can be used for `R` coding independent of any device? Where all you need is a decent browser, a working internet connection and you're good to go?
 
Indeed it would be! (I love rhethorical questions...) In this post, I will show you the steps for setting up such an environment on Amazon Web Services (AWS). The main advantages of using such a set-up:
\itemize
\item Runs on any infrastructure: All you need is a working internet connection, a decent browser and an AWS account, which is usually free.*Footnote: For a given value of usually. I personally try to test out lots of resources just because I can, yet even so, my total expenses for AWS result in 0.37€ (January 2018). A related point
\item Runs everywhere: The AWS machine will be set up to automatically clone your GitHub repository (don't worry if this doesn't mean anything to you, this point is optional), so that you don't even have to have your codes on the device.
\item Scalable: The AWS machine running your code can be chosen to suit any of your needs, in any session. Just playing around with a new package? Use the smallest size, doesn't cost a dime. Trying to re-create Google's preformance on a fancy DNN-classification? Go all in with 500 GB of RAM; it'll cost ya, but it's fun.
\item Up-to-Date: Since the envirionment is freshly installed each time, your `R` version as well as the package versions in use are automatically up-to-date. In the latter case, that would also be easy to maintain on a local machine, the former, however, is a nice benefit. 

Convinced? Awesome, let's get started!

## Overview of main steps

First a short overview of the main steps covered in this blog post:

1. Get an AWS account (duh!),
2. Find the right RStudio AMI,
3. Configure the security group for your RStudio instance,
4. Create a launch template in AWS,
4a. Incorporate a clone of your GitHub repo, 
5. Shut down the Instance and all Resources.

Preconditions for this tutorial should be basically none, at least in terms of coding and/or understanding `R` itself. The main task will lie in clicking the right buttons.

### Step 1: Get an AWS account.

Well, it isn't really my place to tell you how to get an AWS account if Amazon itself did such a great job explaining it <https://aws.amazon.com/resources/create-account/>. Just use the link to set up your account, and I further suggest to follow the following set of instructions <https://aws.amazon.com/getting-started/tutorials/launch-a-virtual-machine/>, building your very first instance. Take your time going through these instructions, I'll wait...

Ready? Alright, sweet. Then we continue with 

### Step 2: Find the current RStudio AMI.

AMI stands for Amazon Machine Images, which is like an operating system container. More to the point, in an AMI a Linux distribution can be bundled with addtional software packages tailored to any type of need: web development, accounting (I'm guessing here, but ... sure) and, most importantly, using RStudio. <HERE> you can find a wonderful storage of RStudio AMIs. We use the newest version for the correct geographical zone, in my case Frankfurt:

[![Screenshot_Louis_Anslett](/img/screenshot_louisaslett.png)](http://www.louisaslett.com/RStudio_AMI/)

As you can see, thanks to Louis Anslett’s work, the AMI includes not only the newest version of RStudio but also of `R itself as well as a handful of helpful additional software packages. For instance, Git comes pre-installed, which we will use later on; also `Julia`is installed for those looking to try out the possible future of data science languages. But I’m deviating, let’s get back on track with 

### Step 3: Configure the security groups for your RStudio instance

In AWS, security groups LINK:https://docs.aws.amazon.com/en_en/AWSEC2/latest/UserGuide/using-network-security.html control the access to the machine over the internet (if you don‘t care about how exactly this works and only want to follow the instructions, just skip the next sentences). More precisely, they define which kind of protocols may use which ports on your machine from a given IP range. For example, you can set the access rights for a ssh protocol to be able to connect to your machine on port 22 only from your personal IP address at home. 

In our case, we actually only need access via http protocol, since the RStudio instance will allow log-in via browser interface. Therefore, our security group can be kept quite simple:

[![Screenshot_Security Group](/img/screenshot_security_group.png)](https://docs.aws.amazon.com/en_en/AWSEC2/latest/UserGuide/using-network-security.html)

The IP range can be limited to your own personal IP to ensure the safety of your instance. This precaution could be necessary since only the login page of RStudio stands between the internet and your instance (spooky, huh?). However, since the personal IP usually changes each day (roughly speaking), this becomes a personal question of "privacy vs. convenience". In my case, as you can see, convenience won.

### Step 4: Create a launch template in AWS

We now configure a launch template in AWS to simplify the process of starting up our RStudio instance in the future. This step is not entirely necessary, but I highly recommend it due to its simplicity and benefits. Begin by starting the launch template wizard on the AWS Dashboard Side Panel. The only things necessary for the set-up are the AMI of step 2 and the security groups from step 3 (see how neatly this all ties together? Almost as if somebody thought it through...). The only thing left to do is to choose the type of instance to start by default. I recommend to set the lowest ranking instance by default: It is still capable of running basic analyses, yet it doesn‘t cost a thing. Before clicking the finish button:

[![Screenshot_Finish Button](/img/screenshot_finish_button.png)]

there are two small (optional) additions that can be handled with the so-called "User data" in the advanced details  Why exactly this is called "user data", I'm not entirely sure, but as long as it is useful, I don't really care.

### Step 4a: Resetting the RStudio password

In the documentation of the RStudio AMI we can find the following passage: „It is highly recommended you change the password immediately and an easy means of doing this is explained upon login in the script that is loaded there“. Alright, fine, but it bothers me to have to do that EVERY SINGLE TIME I log on. There‘s a nicer way to do this, and "User data" provides just the framework: All commands placed here get executed at the beginning of the start-up. Thus, we only need to run the following code to change the password for the user RStudio:
CODE
#!/bin/bash
$ echo "rstudio:guest" | sudo chpasswd
CODE,
Unfortunately, AWS XXX understands only BASE64 encoded commands (please don‘t ask me why), we first need to transform this command into the new encoding. This can be done online right here: https://www.base64encode.org/. In Ubuntu, there is also a simple alternative command ´base64‘. We are almost done with the set-up now, there only remains 

### Step 4b (optional): Automatically Clone a GitHub repo

I write all my private code projects on my GitHub account (here: https://github.com/sebastianschweer. What a shameless plug) and I also would like my code to be available for me each time I start up my RStudio instance. Fortunately, this is easily configured with "User data" again, by just adding the command

CODE
git clone https://github.com/sebastianschweer/sastibe.git /home/rstudio/sastibe
CODE
and running the encoding steps as in Step 4a. Now, each time I start up a new RStudio instance, the repo `sastibe` gets cloned into the folder /home/rstudio/sastibe, which is automatically loaded in RStudio. If I alter the code on the instance, I can push my changes to the repo and all that, the connections are already correctly set in the security groups (see Step 3).

Now, we are ready for our first test run. We start our wonderful new machine by hitting the start button:

[![Screenshot_Finish Button](/img/screenshot_finish_button.png)](You max press it now. How exciting!)

AWS now asks us to choose the size of the machine: 

[![Screenshot_Machine Selection](/img/screenshot_xxx)],
And the number of possibilities seems endless. I have created a list of choice examples to use here:

|  Machine Name  | RAM | CPUs | Price |
|:—————:|——————|——————|——|
| t2.micro | 8 GB | No clue | 0 € |
| t2.nano | 8 GB | No clue | 0 € |
| t2.large | 8 GB | No clue | 0 € |

The performance of these machines is benchmarked in an upcoming blogpost <LINK>, but as an overall recommendation I suggest to start with the t2.micro machine. Its performance is good enough for most use cases, yet it is still in the free tier.

After finally launching your instance let’s now check out the `EC2 Instances`, you should see something like this:

[![Screenshot_Finish Button](/img/screenshot_finish_button.png)](You max press it now. How exciting!)

The public IP (in the red circle) is the IP under which the instance can be reached with the browser. Opening the IP in the browser brings us to the following login page:

[![Screenshot_Finish Button](/img/screenshot_finish_button.png)](You max press it now. How exciting!)

And, using our credentials from Step 4a, we finally arrive at the finish line:

[![Screenshot_Finish Button](/img/screenshot_finish_button.png)](You max press it now. How exciting!)

This is now the front-end to a fully functioning Linux environment with the newest versions of R, Python, Julia etc. so that In other words: Congratulations, you now have a state-of-the-art Data Science Environment at your command. Use it wisely ;) If you want to see what kind of wonders you can do with this setup, check out the upcoming blog post YYYY. If you just want to get to know `R`with simple first steps, check out this wonderful introduction: XXXX.

Before you go, though, let me tell you about

### Last Step (After Each AWS Usage): Shutting Down
An AWS instance doesn’t shut down by itself, or go into hibernation or anything like that. It just keeps running unless otherwise specified, eventually costing lots of money (even the free tier services have their prices after some limit). So, let me show you how to shut down your brand new machine. It’s quite simple, just right-click on the running instance and choose *terminate*.

[![Screenshot_Finish Button](/img/screenshot_finish_button.png)](You max press it now. How exciting!)

Since our instance also automatically loaded an EBS volume (like a hard disk to save data), we need to shut that down too. Choose the entry EBS volumes in the sidepane and *Detach* all volumes that are active. If your overview in the Dashboard looks similar to this :

[![Screenshot_Finish Button](/img/screenshot_finish_button.png)](You max press it now. How exciting!)

There are no hidden services running racking up costs.


# Summary

After configuring your AWS environment as decried above, your new ‚Data Science Workflow‘ can look like this:

1. Log in to AWS,
2. Start Launch Template,
3. Choose the Appropriate Machine,
4. Log in to the Machine in the Browser,
5. Do Awesome Data Science,
6. Shut Down Machine and all Resources.

Have fun, and remember: Primere non nocere!



########## Next article ##########

In the post XXXX I have shown you how to setup an AWS instance running the newest RStudio, R, python, Julia and so forth, where the choice of performance of the instance can be freely chosen. However, there is quite a lot of possibilities of instance configurations out there, just check out the drop-down menu on the page itself:

[![Screenshot_Finish Button](/img/screenshot_finish_button.png)](...why is it ‚nano‘, ‚micro‘ but then ‚large‘ ‚extra large‘? Be consistent, dangit!)

These instances differ in two dimensions: price and performance. Obviously, these dimensions are highly correlated, since higher price means (or should mean, at least) higher performance. 

Now, price is easily measured, yet performance is a bit trickier: For example, it is not entirely straightforward to assess the impact of higher RAM, CPU or even GPU directly across many different configurations. But we’re doing data science, right? So why not create a programmatic test in order to gauge the performance empirically? Well, let‘s do it!

## The Test
For this benchmark test I chose the following task for each of the instances to complete: Using the `keras`Library of YYY, classify the sentiment in 100000 imdb recessions into positive and negative and display the results. The code for the benchmark test can be downloaded here UZZZ was taken entirely from this wonderful blog post: ZZZ For those interested, the script trains a 3-layer (16,64,16) feed-forward Neural Network on the vectorized text data, and additionally calculates the metrics on a validation data set for each epoch. 

I Supplied the original code with a little addition: The last lines read
```{r}
Print(difftime(xx,yay) 
```
So that each run of the script returns N measurements:
1. Time: The time elapsed since starting the script (excluding the time to install the libraries and download of the data),
2. Accuracy of model,
3. Loss of model.

## The Candidates

AWS provides a large number of different configurations, and I will not discuss all of these in this post. Rather, let me focus on four different specifications of computing resource demands and chose a distinctive representative:
- General: t2
- Compute Optimization: c5
- Memory Optimized: x1
- Accelerated Computing: p2

For each of these classes, I had planned to test the sizes small, medium, large, xlarge and 2xlarge. The sizes micro, small and medium are actually only available for t2 (oh, no!), so that I ended up only testing 14 configurations. 

## The results

I started with the candidate `t2.small`. Unfortunately, after the installation the script ran into the following error

[![Screenshot_Finish Button](/img/screenshot_finish_button.png)](Huh.)

The error essentially says “Not enough RAM on the machine”. I decided not to alter the script for the test in order to accommodate for smaller RAM sizes (I could have limited the number of used words in the model, for instance) since this test is aimed at situations where scalability is important. A “not possible” result is still a useful result for choosing the right infrastructure.

A similar thing happened in t2.medium, here the RAM was depleted by the time the validation vectors were defined. In the  Anyways, let’s proceed with the results, first in plain numbers:

```{r echo=FALSE}
benchmark_df <- 
data.frame( instance_class = c(“t2”, “t2”, “t2”, “t2”, “t2”, “c5“, “c5“, “c5“, “x1“, “x1“, “x1“, “p2“, “p2“, “p2“,) 
            instance_name  = c(“small”, “medium”, “large”, “xlarge”, “2xlarge”, “large”, “xlarge”, “2xlarge”, “large”, “xlarge”, “2xlarge”, “large”, “xlarge”, “2xlarge”) 
            price_per_hour = c(0.0268, 0.0536, 0.1072, 0.2144, 0.4288, NA, NA, NA, NA, NA, NA, 
            duration_bm    = c(NA, NA, 0.04707, 0.0118866, 0.0114697,
            loss_bm        = c(NA, NA, NA, 0.5165, 0.5150,
            accuracy_bm    = c(NA, NA, NA, 0.8567, 0.8572,
stringsAsFactors = FALSE)
``` 